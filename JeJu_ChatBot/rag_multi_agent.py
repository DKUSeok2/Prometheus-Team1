from typing import Annotated, Sequence, TypedDict, List, Tuple
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langgraph.graph import END, StateGraph
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import time
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

console = Console()

# ìƒíƒœ íƒ€ì… ì •ì˜
class AgentState(TypedDict):
    messages: Sequence[BaseMessage]
    context: str
    current_answer: str
    agent_outputs: List[str]
    final_answer: str

# ì œì£¼ë„ ê´€ê´‘ì§€ ì •ë³´ëŠ” ì´ì „ê³¼ ë™ì¼...
jeju_data = """
ì œì£¼ë„ëŠ” í•œêµ­ì˜ ê°€ì¥ í° ì„¬ìœ¼ë¡œ, ì•„ë¦„ë‹¤ìš´ ìì—°ê³¼ ë…íŠ¹í•œ ë¬¸í™”ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ì£¼ìš” ê´€ê´‘ì§€:
1. í•œë¼ì‚°
- ë†’ì´: 1,947m
- íŠ¹ì§•: ì œì£¼ë„ì˜ ìƒì§•ì ì¸ í™”ì‚°ìœ¼ë¡œ, ìœ ë„¤ìŠ¤ì½” ì„¸ê³„ìì—°ìœ ì‚°ìœ¼ë¡œ ë“±ì¬
- í™œë™: ë“±ì‚°, ì•¼ê°„ ë³„ë³´ê¸°

2. ì„±ì‚°ì¼ì¶œë´‰
- ë†’ì´: 182m
- íŠ¹ì§•: í™”ì‚° ë¶„ì¶œë¡œ í˜•ì„±ëœ ë…íŠ¹í•œ ì§€í˜•
- í™œë™: ì¼ì¶œ ê°ìƒ, ë“±ì‚°

3. ë§Œì¥êµ´
- ê¸¸ì´: ì•½ 7.4km
- íŠ¹ì§•: ì„¸ê³„ ìµœì¥ì˜ ìš©ì•”ë™êµ´
- í™œë™: ë™êµ´ íƒí—˜, ì§€ì§ˆí•™ í•™ìŠµ

4. ìš°ë„
- íŠ¹ì§•: ì†Œì˜ ëª¨ì–‘ì„ ë‹®ì€ ì„¬
- í™œë™: ìì „ê±° ì—¬í–‰, í•´ìˆ˜ìš•

5. ì œì£¼ ì˜¬ë ˆê¸¸
- ì´ ê¸¸ì´: 425km
- íŠ¹ì§•: ì œì£¼ë„ë¥¼ ë‘˜ëŸ¬ì‹¸ëŠ” ê±·ê¸° ì—¬í–‰ê¸¸
- í™œë™: íŠ¸ë ˆí‚¹, ìì—° ê°ìƒ
"""  # ì´ì „ ë°ì´í„°ì™€ ë™ì¼

def show_agent_progress(agent_name: str, message: str):
    """ì—ì´ì „íŠ¸ ì§„í–‰ìƒí™© í‘œì‹œ"""
    console.print(Panel(
        f"[bold blue]{message}[/bold blue]",
        title=f"ğŸ¤– {agent_name}",
        border_style="blue"
    ))

def setup_vectorstore():
    """ë²¡í„° DB ì„¤ì • (ì´ì „ê³¼ ë™ì¼)"""
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        transient=True,
    ) as progress:
        progress.add_task("ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...", total=None)
        # ì´ì „ setup_vectorstore ì½”ë“œì™€ ë™ì¼
        text_splitter = CharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        texts = text_splitter.split_text(jeju_data)
        
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        vectorstore = Chroma.from_texts(
            texts=texts,
            embedding=embeddings,
            persist_directory="./jeju_db"
        )
        return vectorstore

def search_agent(state: AgentState) -> AgentState:
    """ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
    show_agent_progress("Search Agent ğŸ”", "ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    
    vectorstore = setup_vectorstore()
    last_message = state["messages"][-1].content
    
    # k ê°’ì„ ì¦ê°€ì‹œí‚¤ê³  ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
    docs = vectorstore.similarity_search(last_message, k=3)
    context = "\n".join(doc.page_content for doc in docs)
    
    if not context.strip():
        context = jeju_data  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
    
    show_agent_progress("Search Agent ğŸ”", "ê²€ìƒ‰ ì™„ë£Œ!")
    
    return {
        **state,
        "context": context,
        "agent_outputs": state["agent_outputs"] + ["ê²€ìƒ‰ ê²°ê³¼ ì°¾ìŒ"]
    }

def analysis_agent(state: AgentState) -> AgentState:
    """ë¶„ì„ ì—ì´ì „íŠ¸"""
    show_agent_progress("Analysis Agent ğŸ“Š", "ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    time.sleep(1)
    
    # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    context = state["context"]
    question = state["messages"][-1].content
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
    keywords = ["ë†’ì´", "ê¸¸ì´", "íŠ¹ì§•", "í™œë™"]
    analyzed_info = []
    for keyword in keywords:
        if keyword in context:
            analyzed_info.append(f"{keyword} ì •ë³´ ìˆìŒ")
    
    show_agent_progress("Analysis Agent ğŸ“Š", "ë¶„ì„ ì™„ë£Œ!")
    
    return {
        **state,
        "agent_outputs": state["agent_outputs"] + [f"ë¶„ì„ëœ ì •ë³´: {', '.join(analyzed_info)}"]
    }

def generation_agent(state: AgentState) -> AgentState:
    """ìƒì„± ì—ì´ì „íŠ¸"""
    show_agent_progress("Generation Agent ğŸ’­", "ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    
    if not state['context'].strip():
        return {
            **state,
            "current_answer": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "agent_outputs": state["agent_outputs"] + ["ë‹µë³€ ìƒì„± ì‹¤íŒ¨"],
            "final_answer": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # GPT-3.5-turbo ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.3,
        max_tokens=500
    )
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìˆ˜ì •
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì œì£¼ë„ ê´€ê´‘ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ê·œì¹™:
1. ì£¼ì–´ì§„ ì •ë³´ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
3. ì •ë³´ê°€ ì—†ë‹¤ë©´ "ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”"""),
        ("human", """ë‹¤ìŒì€ ì œì£¼ë„ ê´€ê´‘ ì •ë³´ì…ë‹ˆë‹¤:
{context}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.""")
    ])
    
    try:
        chain = prompt | llm
        response = chain.invoke({
            "context": state['context'],
            "question": state['messages'][-1].content
        })
        answer = response.content
    except Exception as e:
        print(f"\n[ì—ëŸ¬] ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        answer = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    show_agent_progress("Generation Agent ğŸ’­", "ë‹µë³€ ìƒì„± ì™„ë£Œ!")
    
    return {
        **state,
        "current_answer": answer,
        "agent_outputs": state["agent_outputs"] + ["ë‹µë³€ ìƒì„±ë¨"],
        "final_answer": answer
    }

def should_continue(state: AgentState) -> bool:
    """ë‹µë³€ì´ ì¶©ë¶„í•œì§€ í™•ì¸"""
    # í•­ìƒ Falseë¥¼ ë°˜í™˜í•˜ì—¬ í•œ ë²ˆì˜ ì‚¬ì´í´ë§Œ ì‹¤í–‰ë˜ë„ë¡ ìˆ˜ì •
    return False

def create_graph():
    """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("searcher", search_agent)
    workflow.add_node("analyzer", analysis_agent)
    workflow.add_node("generator", generation_agent)
    
    # ì—£ì§€ ì„¤ì •
    workflow.add_edge("searcher", "analyzer")
    workflow.add_edge("analyzer", "generator")
    workflow.add_conditional_edges(
        "generator",
        should_continue,
        {
            True: "searcher",
            False: END
        }
    )
    
    workflow.set_entry_point("searcher")
    
    return workflow.compile()

def main():
    console.print("[bold green]ğŸ¤– ì œì£¼ë„ ê´€ê´‘ ì •ë³´ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì‹œì‘![/bold green]")
    console.print("=" * 50)
    
    chain = create_graph()
    
    while True:
        question = input("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ")
        if question.lower() == 'q':
            break
        
        console.print("\n[bold yellow]ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...[/bold yellow]")
        console.print("=" * 50)
        
        state = {
            "messages": [HumanMessage(content=question)],
            "context": "",
            "current_answer": "",
            "agent_outputs": [],
            "final_answer": ""
        }
        
        result = chain.invoke(state)
        
        console.print("\n[bold green]ìµœì¢… ë‹µë³€:[/bold green]")
        console.print(Panel(result["final_answer"], border_style="green"))
        console.print("=" * 50)

def measure_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        # ì‹œì‘ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì¸¡ì •
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_cpu = psutil.cpu_percent()
        
        # í•¨ìˆ˜ ì‹¤í–‰
        result = func(*args, **kwargs)
        
        # ì¢…ë£Œ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì¸¡ì •
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        end_cpu = psutil.cpu_percent()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"Function: {func.__name__}")
        print(f"Execution time: {end_time - start_time:.2f} seconds")
        print(f"Memory usage: {end_memory - start_memory:.2f} MB")
        print(f"CPU usage: {end_cpu:.2f}%")
        
        return result
    return wrapper

if __name__ == "__main__":
    main() 